{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest/XGboost prediction of C<sub>Q</sub> with DFT computed <sup>27</sup>Al tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFT computed tensors was calculated to produce corresponding DFT CQ as the label. Structural and elemental features are derived from the materials' geometry optimized structural information (using VASP and customized python code in /src). The hyper parameters of the machine learning models are optimized using randomgridsearch from sklearn package. This note book is divided into 3 sections:\n",
    "\n",
    "1. ETL of original tensor and structural information.\n",
    "2. EDA and visualizations of the original data.\n",
    "3. Model training and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymatgen.core.structure import Structure as ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ETL of original tensor and structural information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1.1 Get structures and NMR raw tensors.\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Alnmr.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    print(\"length of file is {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if there's any structure dosen't contain the target atom ('Al') and does not contain a structure section\n",
    "problem_compound = []\n",
    "for compound in data:\n",
    "    if \"structure\" not in compound.keys():\n",
    "        problem_compound.append(compound)\n",
    "        continue\n",
    "    sites = []\n",
    "    for site in compound[\"structure\"][\"sites\"]:\n",
    "        sites.append(site[\"label\"])\n",
    "    if \"Al\" not in sites:\n",
    "        problem_compound.append(compound)\n",
    "print(\"num of problem compound:\", len(problem_compound))\n",
    "\n",
    "for compound in problem_compound:\n",
    "    data.remove(compound)\n",
    "print(\"len of none problematic data:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get rid of the redundances\n",
    "for i in range(len(data)):\n",
    "    string = json.dumps(data[i], sort_keys=True)\n",
    "    data[i] = string\n",
    "data = list(set(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    dictionary = json.loads(data[i])\n",
    "    data[i] = dictionary\n",
    "print(\"length of file without redundancy is {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the structure_tensors obj\n",
    "from src.structure_tensors_gen import get_structure_tensors\n",
    "\n",
    "structure_tensors = get_structure_tensors(data)\n",
    "print(\"length of structure_tensors:\", len(structure_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's what a structure_tensors obj looks like:\n",
    "structure_tensors[0][\"tensors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data cleaning and pre-training preparations \n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.structure_tensors_modifier import (\n",
    "    get_n_coord_tensors,\n",
    "    append_coord_num,\n",
    "    add_oxi_state_by_guess,\n",
    ")\n",
    "\n",
    "# Add oxidation state for each structures in structure_tensors obj. Might take a long time based on the structure.\n",
    "structure_tensors = add_oxi_state_by_guess(structure_tensors)\n",
    "# Filter structure based on coordination number and append coord num info to \"tensors\".\n",
    "structure_tensors_filtered = get_n_coord_tensors(structure_tensors, coord=[4, 5, 6])\n",
    "structure_tensors_filtered = append_coord_num(structure_tensors_filtered)\n",
    "len(structure_tensors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chemical environment info to \"Tensor\" list. Might take a long time based on the structure.\n",
    "from src.structure_tensors_modifier import append_ce\n",
    "\n",
    "structure_tensors_filtered = append_ce(structure_tensors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter structures based on local chemenv. Here we select T:4 T:5 O:6 sites\n",
    "from src.structure_tensors_modifier import filter_ce\n",
    "\n",
    "chemenv_filter = filter_ce(structure_tensors_filtered)\n",
    "# number of outliers\n",
    "print(\"number of outliers:\", len(chemenv_filter[\"outliers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data for later direct use\n",
    "processed_data = copy.deepcopy(chemenv_filter[\"filtered\"])\n",
    "for data in processed_data:\n",
    "    data[\"structure\"] = data[\"structure\"].as_dict()\n",
    "dir_ = \"./data/\"\n",
    "filename = \"processed_data_0.5.json\"\n",
    "with open(dir_ + filename, \"w\") as outfile:\n",
    "    json.dump(processed_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA and visualizations of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read processed data and continue\n",
    "with open(\"./data/processed_data_0.5.json\", \"r\") as file:\n",
    "    data_reload = json.load(file)\n",
    "for data in data_reload:\n",
    "    data[\"structure\"] = ST.from_dict(data[\"structure\"])\n",
    "print(\"number of structures:\", len(data_reload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the structural and elemental features.\n",
    "from src.Utility import features_gen\n",
    "\n",
    "nmr_struc_data = features_gen(data_reload)\n",
    "nmr_struc_data.reset_index(drop=True, inplace=True)\n",
    "nmr_struc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is not balanced in terms of neighbor atoms' types, so we need to determine\n",
    "# the proportion of Al sites that have pure oxygen neighbors or not.\n",
    "def is_O(combo):\n",
    "    if combo == \"O\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "nmr_struc_data.insert(\n",
    "    loc=0, column=\"is_O\", value=nmr_struc_data[\"atom_combination\"].map(is_O)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of is_O sites\n",
    "percent = len(nmr_struc_data[nmr_struc_data[\"is_O\"] == True]) / len(\n",
    "    nmr_struc_data[\"is_O\"]\n",
    ")\n",
    "print(f\"percentage of is_O sites:{percent}\\n\", f\"not is_O sites:{1-percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of CQ wrt the types local geometry.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=nmr_struc_data, x=\"CQ\", hue=\"max_ce\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot thr distribution of CQ wrt to is_O.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=nmr_struc_data, x=\"CQ\", hue=\"is_O\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map of structural features\n",
    "feature_rank = [\n",
    "    \"CQ\",\n",
    "    \"fbl_std\",\n",
    "    \"DI\",\n",
    "    \"fba_std\",\n",
    "    \"fba_max\",\n",
    "    \"fba_average\",\n",
    "    \"fbl_max\",\n",
    "    \"fbl_average\",\n",
    "    \"fba_min\",\n",
    "    \"fbl_min\",\n",
    "]\n",
    "heatmap_data = nmr_struc_data.loc[:, \"CQ\":\"DI\"][feature_rank]\n",
    "\n",
    "# rename features for easier understanding\n",
    "feature_rename = {\n",
    "    \"fbl_std\": \"std(fbl)\",\n",
    "    \"fbl_min\": \"min(fbl)\",\n",
    "    \"fba_std\": \"std(fba)\",\n",
    "    \"fba_max\": \"max(fba)\",\n",
    "    \"fba_average\": \"mean(fba)\",\n",
    "    \"fbl_max\": \"max(fbl)\",\n",
    "    \"fbl_average\": \"mean(fbl)\",\n",
    "    \"fba_min\": \"min(fba)\",\n",
    "}\n",
    "heatmap_data.rename(columns=feature_rename, inplace=True)\n",
    "\n",
    "corr = heatmap_data.corr()\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=[15, 12])\n",
    "heat_map = sns.heatmap(\n",
    "    corr,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    cmap=sns.diverging_palette(220, 220, n=200),\n",
    "    # cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True),\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 14},\n",
    ")\n",
    "heat_map.set_xticklabels(\n",
    "    heat_map.get_xticklabels(), rotation=45, horizontalalignment=\"right\"\n",
    ")\n",
    "heat_map.set_yticklabels(\n",
    "    heat_map.get_yticklabels(), rotation=0, horizontalalignment=\"right\"\n",
    ")\n",
    "\n",
    "plt.savefig(\"./figures/27Al_color_map.png\", format=\"png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot without high correlation features: fba_std,fba_max,fba_min,fbl_max,fbl_min\n",
    "\n",
    "try:\n",
    "    corr = corr.drop(\n",
    "        [\"std(fba)\", \"max(fba)\", \"min(fba)\", \"max(fbl)\", \"min(fbl)\"], axis=0\n",
    "    )\n",
    "    corr = corr.drop(\n",
    "        [\"std(fba)\", \"max(fba)\", \"min(fba)\", \"max(fbl)\", \"min(fbl)\"], axis=1\n",
    "    )\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=[10, 8])\n",
    "heat_map = sns.heatmap(\n",
    "    corr,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    cmap=sns.diverging_palette(220, 220, n=200),\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 14},\n",
    ")\n",
    "heat_map.set_xticklabels(\n",
    "    heat_map.get_xticklabels(), rotation=45, horizontalalignment=\"right\"\n",
    ")\n",
    "heat_map.set_yticklabels(\n",
    "    heat_map.get_yticklabels(), rotation=0, horizontalalignment=\"right\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noticed that the distrotion index can not correctly represent some of the local geometries. (red labeled ones)\n",
    "red_labels = nmr_struc_data[(nmr_struc_data[\"DI\"] < 0.001) & (nmr_struc_data[\"CQ\"] > 0)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "sns.scatterplot(data=nmr_struc_data, x=\"DI\", y=\"CQ\", ax=ax)\n",
    "sns.scatterplot(data=red_labels, x=\"DI\", y=\"CQ\", ax=ax, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And these unrepresented geometries are nicely represented by the standard deviation of first order bond length.\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "sns.scatterplot(data=nmr_struc_data, x=\"fbl_std\", y=\"CQ\", ax=ax)\n",
    "sns.scatterplot(data=red_labels, x=\"fbl_std\", y=\"CQ\", ax=ax, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model training and reporting\n",
    "------------------------------------------------\n",
    "1. Random forest\n",
    "2. GBDT (XGboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train test split and rebalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split y and x\n",
    "y = nmr_struc_data[[\"CQ\", \"is_O\"]]\n",
    "x = nmr_struc_data.loc[:, \"fbl_average\":]\n",
    "# x = data_nocollinear\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)\n",
    "\n",
    "print(f\"Size of train set: {len(X_train)}\\nSize of test set: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the dataset using SMOTE to make ti balance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "train = pd.concat([X_train, y_train[\"CQ\"]], axis=1)\n",
    "label = y_train[\"is_O\"]\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.75)\n",
    "under = RandomUnderSampler(sampling_strategy=1.0)\n",
    "steps = [(\"o\", over), (\"u\", under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "train, label = pipeline.fit_resample(train, label)\n",
    "y_train = pd.concat([train[\"CQ\"], label], axis=1)\n",
    "X_train = train.drop(columns=[\"CQ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the data is balanced\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=pd.concat([X_train, y_train], axis=1), x=\"CQ\", hue=\"is_O\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1. Model Training with RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Random Search for Algorithm Tuning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "# create and fit a kernel ridge regression model\n",
    "model = RandomForestRegressor(random_state=10)\n",
    "\n",
    "param = {\n",
    "    \"n_estimators\": randint(low=10, high=1000),\n",
    "    \"max_depth\": randint(low=10, high=50),\n",
    "    \"min_samples_split\": randint(low=2, high=10),\n",
    "    \"min_samples_leaf\": randint(low=1, high=8),\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "grid_rf = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param,\n",
    "    n_iter=10,\n",
    "    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"],\n",
    "    refit=\"r2\",\n",
    "    cv=10,\n",
    "    n_jobs=8,\n",
    ")\n",
    "grid_rf.fit(X_train, y_train[\"CQ\"])\n",
    "\n",
    "# summarize the results of the grid search\n",
    "train_r2_mean = np.sort(grid_rf.cv_results_[\"mean_test_r2\"])[-1]\n",
    "train_RMSE_mean = math.sqrt(\n",
    "    -np.sort(grid_rf.cv_results_[\"mean_test_neg_mean_squared_error\"])[-1]\n",
    ")\n",
    "train_MAE_mean = -np.sort(grid_rf.cv_results_[\"mean_test_neg_mean_absolute_error\"])[-1]\n",
    "\n",
    "print(\n",
    "    \"training score: R2_mean = {}, RMSE_mean = {}, MAE_mean = {}\".format(\n",
    "        train_r2_mean, train_RMSE_mean, train_MAE_mean\n",
    "    )\n",
    ")\n",
    "print(grid_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "feat_importances = pd.Series(\n",
    "    grid_rf.best_estimator_.feature_importances_, index=X_train.columns\n",
    ")\n",
    "feat_importances.nlargest(10).plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print training score\n",
    "y_train_predict = grid_rf.predict(X_train)\n",
    "\n",
    "train_r2 = r2_score(y_train[\"CQ\"], y_train_predict)\n",
    "train_RMSE = math.sqrt(mean_squared_error(y_train[\"CQ\"], y_train_predict))\n",
    "train_MAE = mean_absolute_error(y_train[\"CQ\"], y_train_predict)\n",
    "\n",
    "print(\n",
    "    \"Train scores: R2 = {}, RMSE = {}, MAE = {}\".format(train_r2, train_RMSE, train_MAE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set\n",
    "from datetime import datetime\n",
    "from src.Utility import reg_plot\n",
    "\n",
    "\n",
    "def print_test_results(table):\n",
    "    test_r2 = r2_score(table[\"VASP_CQ\"], table[\"RF_CQ\"])\n",
    "    test_RMSE = math.sqrt(mean_squared_error(table[\"VASP_CQ\"], table[\"RF_CQ\"]))\n",
    "    test_MAE = mean_absolute_error(table[\"VASP_CQ\"], table[\"RF_CQ\"])\n",
    "    print(\n",
    "        \"test scores: R2 = {}, RMSE = {}, MAE = {}\".format(test_r2, test_RMSE, test_MAE)\n",
    "    )\n",
    "\n",
    "\n",
    "y_rf = pd.Series(grid_rf.predict(X_test))\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_result = pd.concat([y_test, y_rf], axis=1)\n",
    "test_result.rename(columns={\"CQ\": \"VASP_CQ\", 0: \"RF_CQ\"}, inplace=True)\n",
    "\n",
    "print_test_results(test_result)\n",
    "\n",
    "\n",
    "# write down the date for png save\n",
    "predict_result = {}\n",
    "predict_result[\"VASP_CQ\"] = y_test[\"CQ\"]\n",
    "predict_result[\"RF_predicted_CQ\"] = y_rf\n",
    "predict_result = pd.DataFrame(predict_result)\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "print(\"date and time:\", dt_string)\n",
    "\n",
    "# plot the correlation\n",
    "reg_plot(\n",
    "    test_result[\"VASP_CQ\"],\n",
    "    test_result[\"RF_CQ\"],\n",
    "    \"VASP calculated CQ (MHz)\",\n",
    "    \"Random Forest predicted CQ (MHz)\",\n",
    ")\n",
    "\n",
    "# # Export y_rf and y_test as .csv\n",
    "# y_output = copy.deepcopy(y_test)\n",
    "# y_output['CQ_rf'] = y_rf\n",
    "# y_output = pd.DataFrame(y_output)\n",
    "# y_output.to_csv('./data/All_feature_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the result between is_O sites and not is_O sites\n",
    "test_result_O = test_result[test_result[\"is_O\"] == True]\n",
    "test_result_notO = test_result[test_result[\"is_O\"] == False]\n",
    "\n",
    "print_test_results(test_result_O)\n",
    "print_test_results(test_result_notO)\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "plot = sns.lmplot(\n",
    "    x=\"VASP_CQ\", y=\"RF_CQ\", data=test_result, hue=\"is_O\", height=6, aspect=5 / 4\n",
    ")\n",
    "plot.set(xlabel=\"VASP calculated CQ (MHz)\", ylabel=\"Random Forest predicted CQ (MHz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2 Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves with respect to max_depth and max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learning curve\n",
    "from src.Utility import learning_curve_param\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    random_state=10,\n",
    "    max_depth=50,\n",
    "    n_estimators=500,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    ")\n",
    "param_name = \"max_features\"\n",
    "# feature_values = np.array(range(1,21))*(X_train.shape[1]//20)\n",
    "param_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "learning_curve_dict = learning_curve_param(\n",
    "    model, X_train, y_train[\"CQ\"], X_test, y_test[\"CQ\"], param_name, param_values\n",
    ")\n",
    "pd.DataFrame(learning_curve_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(learning_curve_dict).to_csv(\n",
    "    \"./data/learning_curve_max_features_structure_all.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves wrt sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a series of smaller datasets randomly selected with size 10% - 100% of the total dataset\n",
    "small_sets = []\n",
    "for p in range(1, 11):\n",
    "    small_sets.append(nmr_struc_data.sample(frac=p / 10, random_state=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Utility import learning_curve_samplesize\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    random_state=10,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    max_depth=50,\n",
    "    n_estimators=500,\n",
    "    max_features=\"sqrt\",\n",
    ")\n",
    "feature_names = small_sets[0].loc[:, \"fbl_average\":].columns\n",
    "learning_curve_dict = learning_curve_samplesize(model, small_sets, feature_names)\n",
    "pd.DataFrame(learning_curve_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(learning_curve_dict).to_csv(\n",
    "    \"./data/learning_curve_samplesize_structure_and_elemental.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 GBDT (XGboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import math\n",
    "\n",
    "model = xgboost.XGBRegressor(tree_method=\"hist\")\n",
    "\n",
    "param = {\n",
    "    \"learning_rate\": uniform(0, 1),\n",
    "    \"max_depth\": randint(3, 50),\n",
    "    \"min_child_weight\": randint(1, 10),\n",
    "    \"eta\": uniform(0.01, 0.2),\n",
    "    \"gamma\": uniform(0, 1),\n",
    "    \"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 100],\n",
    "    \"subsample\": uniform(0, 1),\n",
    "    \"colsample_bytree\": uniform(0, 1),\n",
    "}\n",
    "grid_gb = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param,\n",
    "    n_iter=10,\n",
    "    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"],\n",
    "    refit=\"r2\",\n",
    "    cv=5,\n",
    ")\n",
    "grid_gb.fit(X_train, y_train[\"CQ\"])\n",
    "\n",
    "# summarize the results of the grid search\n",
    "train_r2 = np.sort(grid_gb.cv_results_[\"mean_test_r2\"])[-1]\n",
    "train_RMSE = math.sqrt(\n",
    "    -np.sort(grid_gb.cv_results_[\"mean_test_neg_mean_squared_error\"])[-1]\n",
    ")\n",
    "train_MAE = -np.sort(grid_gb.cv_results_[\"mean_test_neg_mean_absolute_error\"])[-1]\n",
    "\n",
    "print(\n",
    "    \"training score: R2 = {}, RMSE = {}, MAE = {}\".format(\n",
    "        train_r2, train_RMSE, train_MAE\n",
    "    )\n",
    ")\n",
    "print(grid_gb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict test set\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "y_rf = grid_gb.predict(X_test)\n",
    "\n",
    "test_r2 = r2_score(y_test[\"CQ\"], y_rf)\n",
    "test_RMSE = math.sqrt(mean_squared_error(y_test[\"CQ\"], y_rf))\n",
    "test_MAE = mean_absolute_error(y_test[\"CQ\"], y_rf)\n",
    "\n",
    "print(\"test scores: R2 = {}, RMSE = {}, MAE = {}\".format(test_r2, test_RMSE, test_MAE))\n",
    "\n",
    "\n",
    "# write down the date for png save\n",
    "predict_result = {}\n",
    "predict_result[\"VASP_CQ\"] = y_test[\"CQ\"]\n",
    "predict_result[\"RF_predicted_CQ\"] = y_rf\n",
    "predict_result = pd.DataFrame(predict_result)\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "print(\"date and time:\", dt_string)\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot = sns.regplot(\n",
    "    x=\"RF_predicted_CQ\",\n",
    "    y=\"VASP_CQ\",\n",
    "    data=predict_result,\n",
    "    ci=None,\n",
    "    scatter_kws={\"color\": \"black\"},\n",
    "    line_kws={\"color\": \"red\"},\n",
    ")\n",
    "ax.set_xlabel(\"XGBoost predicted CQ (MHz)\", fontsize=20)\n",
    "ax.set_ylabel(\"VASP calculated CQ (MHz)\", fontsize=20)\n",
    "sns.despine()\n",
    "# plt.savefig('./figures/27Al_RF_testSet_{}.png'.format(dt_string))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "\n",
    "dir_ = \"./models/best/\"\n",
    "filename = \"Best_model_060722.sav\"\n",
    "pickle.dump(grid_gb, open(dir_ + filename, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a100d861a3d6438a0153a54f755c32ecbafe1a787960322fb78fe4fcf72b617a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('27al_ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
